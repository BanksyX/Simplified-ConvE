{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7f2be7ab83d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 关于 python 里的map函数\n",
    "'''\n",
    "map会根据提供的函数对指定序列逐一进行映射\n",
    "所以第一个参数function是映射函数，可以用lambda匿名函数实现\n",
    "第二个参数就是iterable序列\n",
    "返回新的序列\n",
    "'''\n",
    "\n",
    "def square(x):\n",
    "    return x**2\n",
    "\n",
    "map(square, [1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这行代码就很简洁的实现了列表中每个元素的操作\n",
    "# 所以map+lambda适合处理列表等序列化元素的操作\n",
    "list(map(lambda x: x**2, [1,2,3,4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5', '6']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: str(x), [1,2,3,4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关于 * 在python中的作用，乘法就不说了\n",
    "# 主要是 *在形参和实参中的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3, 4)\n",
      "{'name': 'Banksy'}\n"
     ]
    }
   ],
   "source": [
    "# 形参中表示可变长参数，即传入不定个数的位置参数，并且是以元组传入，**表示传入不定个数的关键字\n",
    "def func(*args, **kwargs):\n",
    "    print(args)\n",
    "    print(kwargs)\n",
    "\n",
    "func(1,2,3,4, name = 'Banksy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "1 2 3 4\n"
     ]
    }
   ],
   "source": [
    "# 实参中用于参数解包，比如传入一个列表或者元组，前面加*，就是自动解包\n",
    "a = [1,2,3,4]\n",
    "print(a)\n",
    "print(*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(map(lambda x: x*2, [1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (2, 5), (3, 6)]\n",
      "[1, 2, 3]\n",
      "[4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# 关于zip函数\n",
    "# zip函数用于将可迭代的对象作为参数，将对象中的元素打包成一个元组，返回这些元组组成的列表\n",
    "# 如果参数使用了*号操作符，就是将解压操作，其实也就是实参中的解包作用，将元组解包为列表\n",
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "zip1 = zip(a,b)\n",
    "print(list(zip1))\n",
    "a1, b1 = zip(*zip(a,b))\n",
    "print(list(a1))\n",
    "print(list(b1))\n",
    "\n",
    "# 这里的*很适合数据处理操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3, 2, 1, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# list 的 expand 操作\n",
    "a = [1,2,3,4,5]\n",
    "b = [4,3,2,1]\n",
    "b.extend(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关于csv文件操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "data = [\n",
    "    {'name', 'Banksy'},\n",
    "    {'name', 'MU'},\n",
    "    {'name', 'Ling'},\n",
    "    {'name', 'Zhi'},\n",
    "]\n",
    "with open('1.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i in data:\n",
    "        writer.writerow(i)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Banksy', 'name'], ['MU', 'name'], ['name', 'Ling'], ['name', 'Zhi']]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "csv_list = []\n",
    "with open('1.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i in reader:\n",
    "        csv_list.extend([i])\n",
    "    f.close()\n",
    "print(csv_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将原txt数据读入成字典dict形式\n",
    "data_dict.get(s)这里要用get函数，dict.get(key)是返回给定key的value，否则若为空就返回None，因为这里是大字典套小字典，所以我们要先给对象subject初始化一个dict\n",
    "因为返回的data_dict是一个dict，所以如果要遍历这个dict，就用dict.items()，items是一个可迭代对象，常用于dict遍历中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/m/027rn\n",
      "<class 'dict'>\n",
      "12\n",
      "relation:/location/country/form_of_government\n",
      "object:['/m/06cx9', '/m/026wp']\n",
      "relation:/olympics/olympic_participating_country/medals_won./olympics/olympic_medal_honor/olympics\n",
      "object:['/m/0l6ny', '/m/0kbws', '/m/0kbvb', '/m/06sks6']\n",
      "relation:/location/statistical_region/gdp_nominal_per_capita./measurement_unit/dated_money_value/currency\n",
      "object:['/m/09nqf']\n",
      "relation:/olympics/olympic_participating_country/medals_won./olympics/olympic_medal_honor/medal\n",
      "object:['/m/02lq67', '/m/02lq5w', '/m/02lpp7']\n",
      "relation:/base/aareas/schema/administrative_area/administrative_parent\n",
      "object:['/m/02j71']\n",
      "relation:/olympics/olympic_participating_country/athletes./olympics/olympic_athlete_affiliation/olympics\n",
      "object:['/m/06sks6']\n",
      "relation:/location/statistical_region/gdp_nominal./measurement_unit/dated_money_value/currency\n",
      "object:['/m/09nqf']\n",
      "relation:/location/statistical_region/gdp_real./measurement_unit/adjusted_money_value/adjustment_currency\n",
      "object:['/m/09nqf']\n",
      "relation:/base/popstra/location/vacationers./base/popstra/vacation_choice/vacationer\n",
      "object:['/m/0261x8t']\n",
      "relation:/organization/organization_member/member_of./organization/organization_membership/organization\n",
      "object:['/m/07t65', '/m/02vk52z', '/m/04k4l']\n",
      "relation:/location/location/contains\n",
      "object:['/m/0fthl']\n",
      "relation:/location/country/official_language\n",
      "object:['/m/06nm1']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def data_loader(data_path):\n",
    "    data_dict = {}\n",
    "    with open(data_path, 'r') as f:\n",
    "        csv_reader = csv.reader(f, delimiter='\\t')\n",
    "        for s,r,o in csv_reader:\n",
    "            try:\n",
    "                data_dict[s][r].append(o)\n",
    "            except KeyError:\n",
    "                if data_dict.get(s) is None:\n",
    "                    data_dict[s] = dict()\n",
    "                data_dict[s][r] = [o]\n",
    "    return data_dict\n",
    "\n",
    "data = data_loader('./data/train.txt')\n",
    "for s, ro in data.items():\n",
    "    break\n",
    "print(s)\n",
    "print(type(ro))\n",
    "print(len(ro))\n",
    "for r, o in ro.items():\n",
    "    print(\"relation:{0}\\nobject:{1}\".format(r,o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data_dict):\n",
    "    x, y = list(), list()\n",
    "    e2index, index2e, r2index, index2r = dict(), dict(), dict(), dict()\n",
    "    for s, ro in data_dict.items():\n",
    "        try:\n",
    "            _ = e2index[s]\n",
    "        except KeyError:\n",
    "            index = len(e2index)\n",
    "            e2index[s] = index\n",
    "            index2e[index] = s\n",
    "        \n",
    "        for r, os in ro.items():\n",
    "            try:\n",
    "                _ = r2index[r]\n",
    "            except KeyError:\n",
    "                index = len(r2index)\n",
    "                r2index[r] = index\n",
    "                index2r[index] = r\n",
    "            \n",
    "            for o in os:\n",
    "                try:\n",
    "                    _ = e2index[o]\n",
    "                except KeyError:\n",
    "                    index = len(e2index)\n",
    "                    e2index[o] = index\n",
    "                    index2e[index] = o\n",
    "            \n",
    "            x.append((s,r))\n",
    "            y.append(os)\n",
    "                    \n",
    "    return x, y, e2index, index2e, r2index, index2r\n",
    "\n",
    "x, y, e2index, index2e, r2index, index2r = build_dataset(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93372\n",
      "('/m/027rn', '/olympics/olympic_participating_country/medals_won./olympics/olympic_medal_honor/olympics')\n",
      "93372\n",
      "['/m/0l6ny', '/m/0kbws', '/m/0kbvb', '/m/06sks6']\n",
      "\n",
      "\n",
      "14505\n",
      "<class 'dict'>\n",
      "index:0\t\tentity:/m/027rn\n",
      "\n",
      "\n",
      "237\n",
      "<class 'dict'>\n",
      "index:0\t\trelation:/location/country/form_of_government\n"
     ]
    }
   ],
   "source": [
    "print(len(x))\n",
    "print(x[1])\n",
    "print(len(y))\n",
    "print(y[1])\n",
    "print('\\n')\n",
    "\n",
    "print(len(e2index))\n",
    "print(type(e2index))\n",
    "for entity, index in e2index.items():\n",
    "    print(\"index:{1}\\t\\tentity:{0}\".format(entity, index))\n",
    "    break\n",
    "print('\\n')\n",
    "\n",
    "print(len(index2r))\n",
    "print(type(index2r))\n",
    "for index, relation in index2r.items():\n",
    "    print(\"index:{0}\\t\\trelation:{1}\".format(index, relation))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#entities:14505 \n",
      "#relations:237 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def preprocess_train(data_path):\n",
    "    data_dict = data_loader(data_path)\n",
    "    x, y, e2index, index2e, r2index, index2r = build_dataset(data_dict)\n",
    "    \n",
    "    data = {\n",
    "        'x': x,\n",
    "        'y': y,\n",
    "        'e2index': e2index,\n",
    "        'index2e': index2e,\n",
    "        'r2index': r2index,\n",
    "        'index2r': index2r\n",
    "    }\n",
    "    \n",
    "    print(\"#entities:{0} \".format(len(e2index)))\n",
    "    print(\"#relations:{0} \".format(len(r2index)))\n",
    "    \n",
    "    save_data_path = os.path.splitext(data_path)[0] + '.pkl'\n",
    "    pickle.dump(data, open(save_data_path, 'wb'))\n",
    "\n",
    "preprocess_train('./data/train.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于__getattr__ 函数的作用\n",
    "如果对对象进行属性查询，没查到失败了，那么就会自动调用类的__getattr__函数\n",
    "如果没有定义这个函数，就会抛出AttributeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other fn: asd\n",
      "default:666\n"
     ]
    }
   ],
   "source": [
    "class A(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    \n",
    "    def mydefault(self, *args):\n",
    "        print('default:' + str(args[0]))\n",
    " \n",
    "    def __getattr__(self, name):\n",
    "        print(\"other fn:\", name)\n",
    "        return self.mydefault\n",
    "\n",
    "a1 = A(10, 20)\n",
    "a1.asd(666) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'util.AttributeDict'>\n",
      "6\n",
      "93372\n",
      "14505\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from util import AttributeDict\n",
    "\n",
    "with open('./data/train.pkl', 'rb') as f:\n",
    "    train_data = AttributeDict(pickle.load(f))\n",
    "\n",
    "with open('./data/train.pkl', 'rb') as f:\n",
    "    train_data1 = pickle.load(f)\n",
    "    \n",
    "print(type(train_data))\n",
    "print(len(train_data))\n",
    "print(len(train_data.x))\n",
    "print(len(train_data.e2index))\n",
    "\n",
    "print(type(train_data1))\n",
    "print(len(train_data1.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于 python 中 __getsttr__ 的使用\n",
    "当调用对象的属性时，如果有该属性就调用，没有这个属性就调用 __getattr__ 函数\n",
    "下面就是关于 util.py 中 AttributeDict的样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banksy Test\n",
      "Hello! I am from Class A\n"
     ]
    }
   ],
   "source": [
    "class A:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def __getitem__(self, key):\n",
    "        return 'Hello! I am from Class A'\n",
    "    \n",
    "class F(A):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    __getattr__ = A.__getitem__\n",
    "    \n",
    "obj = F('Banksy Test')\n",
    "print(obj.name)\n",
    "print(obj.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_valid(train_path, valid_path):\n",
    "    x, y = list(), list()\n",
    "    with open(train_path, 'rb') as f:\n",
    "        train_data = AttributeDict(pickle.load(f))\n",
    "    \n",
    "    data_dict = data_loader(valid_path)\n",
    "    \n",
    "    for s, ro in data_dict.items():\n",
    "        try:\n",
    "            _ = train_data.e2index[s]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        for r, objects in ro.items():\n",
    "            try:\n",
    "                _ = train_data.r2index[r]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            \n",
    "            filtered_objects = list()\n",
    "            \n",
    "            for o in objects:\n",
    "                try:\n",
    "                    _ = train_data.e2index[o]\n",
    "                    filtered_objects.append(o)\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            \n",
    "            x.append((s,r))\n",
    "            y.append(filtered_objects)\n",
    "    \n",
    "    data = {\n",
    "        'x':x,\n",
    "        'y':y,\n",
    "    }\n",
    "    \n",
    "    save_file_path = os.path.splitext(valid_path)[0] + '.pkl'\n",
    "    pickle.dump(data, open(save_file_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_valid('./data/train.pkl', './data/valid.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "<class 'util.AttributeDict'>\n",
      "12072\n",
      "12072\n"
     ]
    }
   ],
   "source": [
    "with open('./data/valid.pkl', 'rb') as f:\n",
    "    valid_data = AttributeDict(pickle.load(f))\n",
    "    \n",
    "print(len(valid_data))\n",
    "print(type(valid_data))\n",
    "print(len(valid_data.x))\n",
    "print(len(valid_data.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description = 'Preprocess knowledge graph csv/txt train/valid data.')\n",
    "    sub_parsers = parser.add_subparsers(help='mode', dest='mode')\n",
    "    sub_parsers.required = True\n",
    "    train_parser = sub_parsers.add_parser('train', help='Preprocess a training set')\n",
    "    valid_parser = sub_parsers.add_parser('valid', help='Preprocess a valid or test set')\n",
    "    \n",
    "    train_parser.add_argument('train_path', type=str, help='Path to the raw train dataset (csv or txt file)')\n",
    "    \n",
    "    valid_parser.add_argument('train_path', type=str, help='Path to preprocessed train dataset (pkl file)')\n",
    "    valid_parser.add_argument('valid_path', type=str, help='Path to raw valid dataset (csv or txt file)')\n",
    "    \n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "    if args.mode == 'train':\n",
    "        preprocess_train(args.train_path)\n",
    "    else:\n",
    "        preprocess_valid(args.train_path, args.valid_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor1 = torch.LongTensor((3,5))\n",
    "print(tensor1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 关于 torch.autograd.grad() function\n",
    "def grad(outputs, inputs, grad_outputs=None, retain_graph=None, ...)\n",
    "下面的例子是对 y = x^2 进行求导， x y都是张量\n",
    "\n",
    "torch.autograd.backward() 和 torch.autograd.grad() 都是torch里用来进行自动求导的\n",
    "二者都需要对 标量求导还是向量求导进行区别，对向量求导时grad_outputs要设置为和torch一样size的向量，就相当于是一个weigh matrix权重矩阵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[0., 1., 2., 3.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [2., 3., 4., 5.]], grad_fn=<CopySlices>)\n",
      "y: tensor([[ 0.,  1.,  4.,  9.],\n",
      "        [ 1.,  4.,  9., 16.],\n",
      "        [ 4.,  9., 16., 25.]], grad_fn=<PowBackward0>)\n",
      "weight: tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[ 0.,  2.,  4.,  6.],\n",
      "        [ 2.,  4.,  6.,  8.],\n",
      "        [ 4.,  6.,  8., 10.]], grad_fn=<MulBackward0>)\n",
      "tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "x = torch.randn(3, 4).requires_grad_(True)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        x[i][j] = i + j\n",
    "y = x ** 2\n",
    "\n",
    "print(\"x: {0}\".format(x))\n",
    "print(\"y: {0}\".format(y))\n",
    "\n",
    "weight = torch.ones(y.size())\n",
    "# 这里 weight matrix 就是和y一样的size\n",
    "\n",
    "print(\"weight: {0}\".format(weight))\n",
    "\n",
    "dydx = torch.autograd.grad(outputs=y,\n",
    "                           inputs=x,\n",
    "                           grad_outputs=weight,\n",
    "                           retain_graph=True,\n",
    "                           create_graph=True,\n",
    "                           only_inputs=True)\n",
    "\"\"\"(x**2)' = 2*x \"\"\"\n",
    "print(dydx[0])\n",
    "\n",
    "d2ydx2 = torch.autograd.grad(outputs=dydx[0],\n",
    "                             inputs=x,\n",
    "                             grad_outputs=weight,\n",
    "                             retain_graph=True,\n",
    "                             create_graph=True,\n",
    "                             only_inputs=True)\n",
    "print(d2ydx2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 关于torch.tensor 和 torch.Tensor 的区别\n",
    "tensor是一个函数，Tensor是一个类\n",
    "两者最后都会产生一个tensor张量数据类型\n",
    "tensor(data, dtype, device, requires_grad)\n",
    "\n",
    "Tensor是一个class，是torch.FloatTensor的别称，所以可见，Tensor类的对象自动就是Float类型\n",
    "\n",
    "而tensor函数产生的是int对象，进行张量运算时都需要特别指定dtype=torch.Float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 1.2000],\n",
      "        [2.3000, 3.2000],\n",
      "        [2.3000, 5.6000]])\n",
      "tensor([[1., 4.],\n",
      "        [3., 4.]], device='cuda:0', dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([\n",
    "    [0.1, 1.2],\n",
    "    [2.3, 3.2],\n",
    "    [2.3, 5.6]\n",
    "])\n",
    "t2 = torch.tensor([\n",
    "    [1, 4],\n",
    "    [3, 4]\n",
    "], dtype=torch.float64, device=torch.device('cuda:0'), requires_grad=True)\n",
    "\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3]\n",
    "T1 = torch.Tensor(data)\n",
    "T1.requires_grad = True\n",
    "print(T1)\n",
    "print(type(T1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# python iter() 用来生成迭代器\n",
    "l1 = [1,2,3,4,5]\n",
    "for i in iter(l1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset import KnowledgeGraphDataset\n",
    "from util import AttributeDict\n",
    "import dataset\n",
    "\n",
    "with open('./data/train.pkl', 'rb') as f:\n",
    "    train_data = AttributeDict(pickle.load(f))\n",
    "\n",
    "train_dataset = DataLoader(\n",
    "    KnowledgeGraphDataset(train_data.x, train_data.y,\n",
    "                          e2index=train_data.e2index,\n",
    "                          r2index=train_data.r2index),\n",
    "    collate_fn=dataset.collate_train, \n",
    "    batch_size=256, \n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 97.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm import trange\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: abc: 100%|██████████| 300/300 [00:00<00:00, 3400.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "pbar = tqdm(range(300))\n",
    "\n",
    "for i in pbar:\n",
    "    err = 'abc'\n",
    "    pbar.set_description(\"Reconstruction loss: {0}\".format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from dataset import KnowledgeGraphDataset\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### torch.nn.Embedding \n",
    "1. nn.Embedding 可以用于词嵌入，就是形成一个映射词典，维护一个这样的映射查找表\n",
    "每个词都会有一个嵌入向量对应，输出就是嵌入向量集合;\n",
    "embed = torch.nn.Embedding(num_vocabulary, embedding_dim)\n",
    "输入为两个参数，词的数量，每个词映射成的维度dim\n",
    "\n",
    "2. 然后，输入Embedding的word type必须是LongTensor类型\n",
    "即， torch.nn.Embedding(torch.LongTensor(word))\n",
    "3. embedding = nn.Embedding(num, dim) 这是声明一个 Embedding Table，也就是embedding就是一个embedding table，这其实就是实现了一个Embedding weight\n",
    "4. 然后 embed_vector = embedding(input_data) 实际的操作就是将Embedding weight和input_data进行相乘，得到embedded vector，这里实际就是通过一个简单的多层神经网络实现的运算，no bias & no activation function，得到的embedded vector 本质上是一个稠密的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6385, -0.6395, -0.7924,  0.2629, -0.0947]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "word2id = {'hello': 0, 'world': 1}\n",
    "embeds = nn.Embedding(2,5)\n",
    "hello_id = torch.LongTensor([word2id['hello']])\n",
    "#hello_id = Variable(hello_id)\n",
    "'''\n",
    "以前的版本吧这里还需要，对Tensor进行一次Variable封装，但是现在不需要了，Variable本身已经集成到了Tensor里面去了，所以直接用前面的LongTensor就可以了\n",
    "'''\n",
    "\n",
    "hello_embed = embeds(hello_id)\n",
    "print(hello_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 4])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.Tensor(3,4).long()\n",
    "print(t1.shape)\n",
    "print(type(t1))\n",
    "\n",
    "t2 = torch.LongTensor(3,4)\n",
    "print(t2.shape)\n",
    "print(type(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "embedding weight:\n",
      "Parameter containing:\n",
      "tensor([[-0.1707,  0.5197,  0.2059,  0.4245],\n",
      "        [-0.5471, -0.1096,  0.9400, -0.2641],\n",
      "        [ 0.3535, -0.0306,  0.0616,  2.0480],\n",
      "        [ 0.3477, -0.3344,  1.6420, -1.2653],\n",
      "        [ 1.5042,  0.7121,  0.3164, -0.0639],\n",
      "        [-0.3180, -1.4425, -1.6027, -0.3420]], requires_grad=True)\n",
      "tensor([[-0.1707, -0.5471,  0.3535,  0.3477,  1.5042, -0.3180],\n",
      "        [ 0.5197, -0.1096, -0.0306, -0.3344,  0.7121, -1.4425],\n",
      "        [ 0.2059,  0.9400,  0.0616,  1.6420,  0.3164, -1.6027],\n",
      "        [ 0.4245, -0.2641,  2.0480, -1.2653, -0.0639, -0.3420]],\n",
      "       grad_fn=<TBackward>)\n",
      "word1:\n",
      "tensor([[-0.1707,  0.5197,  0.2059,  0.4245],\n",
      "        [-0.5471, -0.1096,  0.9400, -0.2641],\n",
      "        [ 0.3535, -0.0306,  0.0616,  2.0480]], grad_fn=<EmbeddingBackward>)\n",
      "word2:\n",
      "tensor([[ 0.3477, -0.3344,  1.6420, -1.2653],\n",
      "        [-0.3180, -1.4425, -1.6027, -0.3420],\n",
      "        [ 0.3477, -0.3344,  1.6420, -1.2653]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "word1 = torch.LongTensor([0, 1, 2])\n",
    "word2 = torch.LongTensor([3, 5, 3])\n",
    "embedding = nn.Embedding(6, 4)\n",
    "\n",
    "print('\\nembedding weight:')\n",
    "print(embedding.weight)\n",
    "print(embedding.weight.t())\n",
    "# 这里的embedding.weight 是每次随机生成的\n",
    "\n",
    "print('word1:')\n",
    "print(embedding(word1))\n",
    "print('word2:')\n",
    "print(embedding(word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 关于tensor调整size的方法，view、squeeze、resize\n",
    "1. view() 方法\n",
    "view必须保证形状调整前后元素总数一致，不会修改数据本身，且和原tensor共享内存，更改一个，另外一个随之改变\n",
    "2. sequeeze是减少一个维度，unsequeeze是添加一个维度\n",
    "3. resize() 也是修改tensor尺寸，但是和view不同在于，resize会自动重新分配内存，数据也可能会发生变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(0,6).view(2, 3)\n",
    "print(a)\n",
    "\n",
    "b = a.view(-1, 2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[[0, 1, 2]],\n",
      "\n",
      "        [[3, 4, 5]]])\n",
      "torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 6).view(2, 3)\n",
    "print(a)\n",
    "b = a.view(-1, 3)\n",
    "print(b)\n",
    "print(b.size())\n",
    "c = b.unsqueeze(1)\n",
    "print(c)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 关于 item() 可以从下面看出，对于一个tensor，如果单纯的获取对应元素，那么这个元素仍然属于tensor类型，但是如果是通过item()获取就得到了一个元素，而且精度更高\n",
    "在求loss的时候，我们常通过loss.item()来获取具体loss值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2525, 0.8337],\n",
      "        [0.4825, 0.3356]])\n",
      "tensor(0.3356)\n",
      "0.3356214761734009\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand(2, 2)\n",
    "print(t1)\n",
    "print(t1[1][1])\n",
    "print(t1[1][1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 关于torch scatter_ 这个常用来做one-hot编码，输入三个参数\n",
    "* 第一个参数是dim，沿哪个维度进行操作，0是列向量，1是行向量\n",
    "* 第二个参数是index，用来进行索引的，必须也是tensor类型\n",
    "* 第三个参数是src，是一个标量或者张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[3., 3., 3., 3., 3.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [3., 3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.zeros(3,5)\n",
    "print(t1)\n",
    "t1.scatter_(0, torch.tensor([[0, 0, 0, 0, 0], [2, 2, 2, 2, 2]]), 3)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9],\n",
      "        [1],\n",
      "        [9],\n",
      "        [2]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "batch_size, dim_num = 4, 10\n",
    "# 下面这行代码，在训练模型时很常用到，常用来生成one-hot编码，生成一个batch_size * 1 的label，dim=1的情况下\n",
    "label = torch.LongTensor(batch_size, 1).random_() % dim_num\n",
    "print(label)\n",
    "t1_onehot = torch.zeros(batch_size, dim_num).scatter_(1, label, 1)\n",
    "print(t1_onehot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19292/3585921101.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0my_multihot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me2index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjects\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/zhu/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/zhu/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/zhu/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/xhd/ConvE-simplify/mySimpConvE/dataset.py\u001b[0m in \u001b[0;36mcollate_train\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a map-style dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollate_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/xhd/ConvE-simplify/mySimpConvE/dataset.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a map-style dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollate_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from util import AttributeDict\n",
    "from dataset import KnowledgeGraphDataset\n",
    "import dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "batchsize = 256\n",
    "\n",
    "with open('./data/train.pkl', 'rb') as f:\n",
    "    data = AttributeDict(pickle.load(f))\n",
    "    \n",
    "datasource = KnowledgeGraphDataset(data.x, data.y, e2index=data.e2index, r2index=data.r2index)\n",
    "train_dataset = DataLoader(datasource, collate_fn=dataset.collate_train, batch_size=batchsize,  shuffle=True)\n",
    "\n",
    "moving_loss = 0\n",
    "\n",
    "y_multihot = torch.LongTensor(batch_size, len(data.e2index))\n",
    "\n",
    "# for s, r, objects in iter(train_dataset):\n",
    "#     print(s)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "93372\n",
      "14505\n",
      "237\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(len(data.x))\n",
    "print(len(data.e2index))\n",
    "print(len(data.r2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93372\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7fafcbb8b490>\n"
     ]
    }
   ],
   "source": [
    "print(len(datasource))\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 14505])\n"
     ]
    }
   ],
   "source": [
    "y_multihot = torch.LongTensor(256, len(data.e2index))\n",
    "print(y_multihot.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aaa1454568afae0c45e11cd0566c2a5b91797a96840a90bd908d438693b5d556"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('xhd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
